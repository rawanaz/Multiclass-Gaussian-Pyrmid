{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce02568-f20b-476e-bfe0-1f217f54cdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from matplotlib import cm\n",
    "from scipy import interp\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from itertools import cycle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "start_time = time.perf_counter ()\n",
    "\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier \n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "# data = pd.read_csv(r\"C:\\Users\\Rawan\\Desktop\\data multiclass\\har\\train.csv\")\n",
    "data = pd.read_csv(r\"C:\\Users\\Rawan\\Desktop\\second paper\\dataset\\CTG\\CTG.csv\")\n",
    "# data = pd.read_csv(r\"C:\\Users\\Rawan\\Desktop\\second paper\\dataset\\Sensorless_drive_diagnosis\\Sensorless_drive_diagnosis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203fa0c7-63a7-40d0-ae8a-1470c3183c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = input(\"Enter your Kernel dimensions: \")\n",
    "dim=int(dim)\n",
    "dim_col=dim_row=dim\n",
    "kernel=np.round(np.random.uniform(low=0, high=1, size=(dim,dim)),2)\n",
    "print(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced3fa99-9093-4455-a25a-f0cf5d59d4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data)\n",
    "def Kernel_function(data,kernel,dim_row,dim_col,target):\n",
    "    # print(data)\n",
    "    data =data.drop(['Class'],axis=1)\n",
    "    # With normalization\n",
    "    # x = data.values #returns a numpy array\n",
    "    # min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    # x_scaled = min_max_scaler.fit_transform(x)\n",
    "    # data = pd.DataFrame(x_scaled)\n",
    "    #remove header\n",
    "    data.rename(columns=data.iloc[0]).drop(data.index[0])\n",
    "#     data.columns = data.iloc[0]\n",
    "#     data = data[1:]\n",
    "    # computing number of rows\n",
    "    rows = len(data.axes[0])\n",
    "\n",
    "    # computing number of columns\n",
    "    cols = len(data.axes[1])\n",
    "\n",
    "#     print(\"Number of Rows: \", rows)\n",
    "#     print(\"Number of Columns: \", cols)\n",
    "    kernel=pd.DataFrame(kernel)\n",
    "    Reset_kernel=pd.DataFrame(kernel)\n",
    "    start_row=0\n",
    "    stop_row=rows*cols\n",
    "    start_column=0\n",
    "    stop_column=cols\n",
    "    newDF= []\n",
    "#     sumDF= []\n",
    "    kernel_dataframe = pd.DataFrame()\n",
    "    i=0\n",
    "    j=0\n",
    "    x=1\n",
    "    NumColumnsDF=-8\n",
    "    for i in range(data.shape[0]): #iterate over rows\n",
    "        for j in range(data.shape[1]): #iterate over columns\n",
    "            data_cut=data.iloc[start_row:dim_row, start_column:dim_col]\n",
    "            if data_cut.empty:\n",
    "                break\n",
    "            else:\n",
    "#                 print('this example',start_row,dim_row, start_column,dim_col)\n",
    "                #print(data_cut)\n",
    "                start_column+=dim\n",
    "                dim_col+=dim\n",
    "                #Number of Rows\n",
    "                Len_Data_Row=len(data_cut)\n",
    "                Len_Kernel_Row=len(kernel)\n",
    "                # Number of rows to drop\n",
    "                n=Len_Kernel_Row-Len_Data_Row\n",
    "                # Dropping last n rows using drop\n",
    "                if Len_Data_Row<Len_Kernel_Row:\n",
    "                    kernel=kernel.head(-n)\n",
    "                #number of Columns\n",
    "                Len_Data_Column=len(data_cut.columns)\n",
    "                Len_Kernel_Column=len(kernel.columns)\n",
    "                if Len_Data_Column<Len_Kernel_Column:\n",
    "                    kernel=kernel.iloc[: , :Len_Data_Column]\n",
    "                # Multiply two DataFrames\n",
    "                data_cut=round(pd.DataFrame(data=data_cut),2)\n",
    "                kernel = round(pd.DataFrame(data=kernel),2)\n",
    "                # print(data_cut)\n",
    "                # print(kernel.values)\n",
    "                newDF=round(pd.DataFrame(kernel.astype(float).values*data_cut.astype(float).values),2)\n",
    "                # print(newDF) \n",
    "#                 sumDF.append(round(newDF.values.sum(),2))\n",
    "                sumDF=round(newDF.values.sum(),2)        \n",
    "#                 print(sumDF)\n",
    "                kernel=round(Reset_kernel,2)\n",
    "                if start_row==0 and i==0:\n",
    "                    name='col'+str(i)\n",
    "                    # kernel_dataframe=kernel_dataframe.append({name:sumDF}, ignore_index=True)\n",
    "                    kernel_dataframe.loc[len(kernel_dataframe), [name]] = sumDF\n",
    "                    # print(kernel_dataframe)\n",
    "                    i=i+1\n",
    "                elif start_row==0 and i!=0:\n",
    "                    name='col'+str(i)\n",
    "                    insert_index = len(kernel_dataframe.axes[1])\n",
    "                    kernel_dataframe.insert(insert_index,name, sumDF)\n",
    "                    # print(kernel_dataframe)\n",
    "                    i=i+1\n",
    "                    NumColumnsDF=kernel_dataframe.shape[1]\n",
    "                elif start_row!=0:\n",
    "                    name='col'+str(j)\n",
    "                    kernel_dataframe.at[x,name] =sumDF\n",
    "                    # print(kernel_dataframe)\n",
    "                    j=j+1\n",
    "                    NumColumnsDF=NumColumnsDF-1\n",
    "                    if NumColumnsDF==0:\n",
    "                        x=x+1\n",
    "                        NumColumnsDF=kernel_dataframe.shape[1]\n",
    "        dim_col=dim\n",
    "        start_column=0\n",
    "        start_row+=dim\n",
    "        dim_row+=dim\n",
    "   \n",
    "    kernel_dataframe['Class'] = target\n",
    "        \n",
    "    return kernel_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65de3207-7eba-4d94-9ace-4929d20238a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# har\n",
    "sumDFA=Kernel_function(data.query('Class== \"LAYING\"', inplace = False),kernel,dim_row,dim_col,\"LAYING\")\n",
    "sumDFB=Kernel_function(data.query('Class== \"SITTING\"', inplace = False),kernel,dim_row,dim_col,\"SITTING\")\n",
    "sumDFC=Kernel_function(data.query('Class== \"STANDING\"', inplace = False),kernel,dim_row,dim_col,\"STANDING\")\n",
    "sumDFD=Kernel_function(data.query('Class== \"WALKING\"', inplace = False),kernel,dim_row,dim_col,\"WALKING\")\n",
    "sumDFE=Kernel_function(data.query('Class== \"WALKING_DOWNSTAIRS\"', inplace = False),kernel,dim_row,dim_col,\"WALKING_DOWNSTAIRS\")\n",
    "sumDFF=Kernel_function(data.query('Class== \"WALKING_UPSTAIRS\"', inplace = False),kernel,dim_row,dim_col,\"WALKING_UPSTAIRS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc8651-e683-466d-8792-ff1e7d979dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTG\n",
    "# sumDF1=Kernel_function(data.query('Class== 1', inplace = False),kernel,dim_row,dim_col,\"One\")\n",
    "# sumDF2=Kernel_function(data.query('Class== 2', inplace = False),kernel,dim_row,dim_col,\"two\")\n",
    "# sumDF3=Kernel_function(data.query('Class== 3', inplace = False),kernel,dim_row,dim_col,\"three\")\n",
    "# sumDF4=Kernel_function(data.query('Class== 4', inplace = False),kernel,dim_row,dim_col,\"four\")\n",
    "# sumDF5=Kernel_function(data.query('Class== 5', inplace = False),kernel,dim_row,dim_col,\"five\")\n",
    "# sumDF6=Kernel_function(data.query('Class== 6', inplace = False),kernel,dim_row,dim_col,\"six\")\n",
    "# sumDF7=Kernel_function(data.query('Class== 7', inplace = False),kernel,dim_row,dim_col,\"seven\")\n",
    "# sumDF8=Kernel_function(data.query('Class== 8', inplace = False),kernel,dim_row,dim_col,\"eight\")\n",
    "# sumDF9=Kernel_function(data.query('Class== 9', inplace = False),kernel,dim_row,dim_col,\"nine\")\n",
    "# sumDF10=Kernel_function(data.query('Class== 10', inplace = False),kernel,dim_row,dim_col,\"ten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c357dba0-f397-4c58-b3e4-d0171bdc1602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensorless_drive_diagnosis\n",
    "# sumDF1=Kernel_function(data.query('Class== 1', inplace = False),kernel,dim_row,dim_col,\"one\")\n",
    "# sumDF2=Kernel_function(data.query('Class== 2', inplace = False),kernel,dim_row,dim_col,\"two\")\n",
    "# sumDF3=Kernel_function(data.query('Class== 3', inplace = False),kernel,dim_row,dim_col,\"three\")\n",
    "# sumDF4=Kernel_function(data.query('Class== 4', inplace = False),kernel,dim_row,dim_col,\"four\")\n",
    "# sumDF5=Kernel_function(data.query('Class== 5', inplace = False),kernel,dim_row,dim_col,\"five\")\n",
    "# sumDF6=Kernel_function(data.query('Class== 6', inplace = False),kernel,dim_row,dim_col,\"six\")\n",
    "# sumDF7=Kernel_function(data.query('Class== 7', inplace = False),kernel,dim_row,dim_col,\"seven\")\n",
    "# sumDF8=Kernel_function(data.query('Class== 8', inplace = False),kernel,dim_row,dim_col,\"eight\")\n",
    "# sumDF9=Kernel_function(data.query('Class== 9', inplace = False),kernel,dim_row,dim_col,\"nine\")\n",
    "# sumDF10=Kernel_function(data.query('Class== 10', inplace = False),kernel,dim_row,dim_col,\"ten\")\n",
    "# sumDF11=Kernel_function(data.query('Class== 11', inplace = False),kernel,dim_row,dim_col,\"eleven\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6667cfe7-c5c5-4d5e-a43d-af9a61f2c15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTG\n",
    "# frames = [sumDF1, sumDF2,sumDF3,sumDF4,sumDF5,sumDF6,sumDF7,sumDF8,sumDF9,sumDF10]\n",
    "# har\n",
    "frames = [sumDFA, sumDFB,sumDFC,sumDFD,sumDFE,sumDFF]\n",
    "# Sensorless_drive_diagnosis\n",
    "# frames = [sumDF1, sumDF2,sumDF3,sumDF4,sumDF5,sumDF6,sumDF7,sumDF8,sumDF9,sumDF10,sumDF11]\n",
    "\n",
    "result = pd.concat(frames)\n",
    "# display(result)\n",
    "pd.DataFrame(result).to_csv(\"result.csv\", index=False)\n",
    "\n",
    "# result.to_csv(\"result\", encoding='utf-8')\n",
    "corr0 = result.iloc[: , :-1].astype(float).corr()\n",
    "corr0.style.background_gradient(cmap='coolwarm')\n",
    "# 'RdBu_r', 'BrBG_r', & PuOr_r are other good diverging colormaps\n",
    "corr0.style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daaa01c-7e00-4a4c-afe1-73545c9c89fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "data2 =  result.drop(labels='Class',axis=1)\n",
    "mean0 = np.mean(data2, axis=0)\n",
    "cov0 = np.cov(data2, rowvar=False)\n",
    "        \n",
    "pdf0 = multivariate_normal.pdf(data2, mean0, cov0)\n",
    "pd.DataFrame(pdf0).plot(kind='density') \n",
    "F = plt.gcf()\n",
    "Size = F.get_size_inches()\n",
    "F.set_size_inches(Size[0]*2, Size[1]*2, forward=True) \n",
    "end_time = time.perf_counter ()\n",
    "print(end_time - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb88ba-d36c-4e2e-8a10-74308711d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time2 = time.perf_counter ()\n",
    "X_training = result.drop(['Class'],axis=1).values\n",
    "y_training = result[\"Class\"]\n",
    "\n",
    "x_train, x_test, Y_train_label, Y_test_label = train_test_split(X_training, y_training, test_size=0.30, random_state=42,stratify=y_training)\n",
    "# Y_train_label=y_train.astype(object)\n",
    "# Y_test_label=y_test.astype(object)\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# encoding train labels \n",
    "encoder.fit(Y_train_label)\n",
    "Y_train = encoder.transform(Y_train_label)\n",
    "\n",
    "# encoding test labels \n",
    "encoder.fit(Y_test_label)\n",
    "Y_test = encoder.transform(Y_test_label)\n",
    "# Scaling the Train and Test feature set \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(x_train)\n",
    "X_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "params_grid = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "# Performing CV to tune parameters for best SVM fit \n",
    "svm_model = GridSearchCV(SVC(), params_grid, cv=5)\n",
    "svm_model.fit(X_train_scaled, Y_train)\n",
    "# View the accuracy score\n",
    "print('Best score for training data:', svm_model.best_score_,\"\\n\") \n",
    "\n",
    "# View the best parameters for the model found using grid search\n",
    "print('Best C:',svm_model.best_estimator_.C,\"\\n\") \n",
    "print('Best Kernel:',svm_model.best_estimator_.kernel,\"\\n\")\n",
    "print('Best Gamma:',svm_model.best_estimator_.gamma,\"\\n\")\n",
    "\n",
    "final_model = svm_model.best_estimator_\n",
    "Y_pred = final_model.predict(X_test_scaled)\n",
    "Y_pred_label = list(encoder.inverse_transform(Y_pred))\n",
    "# Making the Confusion Matrix\n",
    "#print(pd.crosstab(Y_test_label, Y_pred_label, rownames=['Actual Activity'], colnames=['Predicted Activity']))\n",
    "print(confusion_matrix(Y_test_label,Y_pred_label))\n",
    "print(\"\\n\")\n",
    "print(classification_report(Y_test_label,Y_pred_label))\n",
    "\n",
    "print(\"Training set score for SVM: %f\" % final_model.score(X_train_scaled , Y_train))\n",
    "print(\"Testing  set score for SVM: %f\" % final_model.score(X_test_scaled  , Y_test ))\n",
    "\n",
    "svm_model.score\n",
    "end_time2 = time.perf_counter ()\n",
    "print(end_time2 - start_time2, \"seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
